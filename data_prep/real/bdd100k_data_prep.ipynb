{"cells":[{"cell_type":"markdown","metadata":{"id":"iIgdBwunBpBc"},"source":["# COMP4471 Project\n","\n","**Deblurring Rainy Driving Images Using Diffusion Models**  \n","Lam King Cheuk, Yau Ho Yin, Chan Chun Hugo  \n","\n"]},{"cell_type":"markdown","metadata":{"id":"P7GiQOfXBs7H"},"source":["# Mount Drive"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2639,"status":"ok","timestamp":1764753818176,"user":{"displayName":"Lam Kingsley","userId":"13349381949172420159"},"user_tz":-480},"id":"mwMz8TwKA414","outputId":"bee4bf95-588b-4b01-ee88-a12db8823786"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","âœ“ Working directory: /content/drive/MyDrive/comp4471/project\n","Working in: /content/drive/MyDrive/comp4471/project\n","total 86\n","drwx------ 8 root root  4096 Dec  3 08:23 data\n","-rw------- 1 root root 83775 Dec  3 09:22 deRain.ipynb\n"]}],"source":["# ============================================================================\n","# SECTION 1: MOUNT GOOGLE DRIVE\n","# ============================================================================\n","# Run this first to access your Google Drive storage\n","\n","from google.colab import drive\n","import os\n","\n","# Mount drive (will skip if already mounted)\n","try:\n","    drive.mount(\"/content/drive\")\n","except:\n","    print(\"Drive already mounted or mount failed\")\n","\n","# Navigate to your project folder (create if doesn't exist)\n","project_dir = \"/content/drive/MyDrive/comp4471/project\"\n","os.makedirs(project_dir, exist_ok=True)\n","os.chdir(project_dir)\n","print(f\"âœ“ Working directory: {os.getcwd()}\")\n","\n","print(f\"Working in: {os.getcwd()}\")\n","!ls -la # Should show your existing notebook/files"]},{"cell_type":"markdown","metadata":{"id":"2SsTh5xgGPRh"},"source":["# Install Dependencies\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qV3_6CyQGOJS","executionInfo":{"status":"ok","timestamp":1764740079595,"user_tz":-480,"elapsed":26,"user":{"displayName":"Lam Kingsley","userId":"13349381949172420159"}},"outputId":"2cb5c13f-ebf7-4c53-8c12-0c2adc7803e4"},"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","CHECKING AND INSTALLING DEPENDENCIES\n","============================================================\n","âœ“ tqdm already installed\n","âœ“ pillow already installed\n","âœ“ opencv-python already installed\n","âœ“ scikit-image already installed\n","\n","âœ“ All dependencies ready\n"]}],"source":["# ============================================================================\n","# SECTION 2: INSTALL DEPENDENCIES\n","# ============================================================================\n","# Install only necessary packages for data preparation\n","\n","print(\"=\" * 60)\n","print(\"CHECKING AND INSTALLING DEPENDENCIES\")\n","print(\"=\" * 60)\n","\n","# List of (pip_package_name, import_name) tuples\n","packages_to_check = [\n","    (\"tqdm\", \"tqdm\"),\n","    (\"pillow\", \"PIL\"),\n","    (\"opencv-python\", \"cv2\"),\n","    (\"scikit-image\", \"skimage\"),\n","]\n","\n","for pip_name, import_name in packages_to_check:\n","    try:\n","        __import__(import_name)\n","        print(f\"âœ“ {pip_name} already installed\")\n","    except ImportError:\n","        print(f\"ðŸ“¥ Installing {pip_name}...\")\n","        get_ipython().system(f\"pip install -q {pip_name}\")\n","        print(f\"âœ“ {pip_name} installed\")\n","\n","print(\"\\nâœ“ All dependencies ready\")"]},{"cell_type":"markdown","metadata":{"id":"z58KRykzGVUd"},"source":["# Dataset Preparation"]},{"cell_type":"markdown","source":["## 1. Data Unzipping"],"metadata":{"id":"3t1B3gZDC0uI"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1745850,"status":"ok","timestamp":1764741830366,"user":{"displayName":"Lam Kingsley","userId":"13349381949172420159"},"user_tz":-480},"id":"efYsqitlGYbU","outputId":"31e98462-8f33-492f-b43d-0c65e4c6c8b2"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","============================================================\n","UNZIPPING BDD100K IMAGES (100K)\n","============================================================\n","ðŸ“¦ Extracting: bdd100k_images_100k.zip\n","ðŸ“‚ To: 100k_images\n","ðŸ“Š Total files in archive: 100,004\n"]},{"output_type":"stream","name":"stderr","text":["Extracting: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100004/100004 [29:03<00:00, 57.35files/s]\n"]},{"output_type":"stream","name":"stdout","text":["âœ“ Extraction complete!\n","âœ“ Images unzipped successfully to 100k_images/\n","\n","ðŸ“ Current structure:\n","total 5.5G\n","drwx------ 3 root root 4.0K Dec  3 05:34 100k_images\n","-rw------- 1 root root 5.3G Dec  3 04:52 bdd100k_images_100k.zip\n","-rw------- 1 root root 181M Dec  3 04:42 bdd100k_labels.zip\n"]}],"source":["# ============================================================================\n","# SECTION 3: UNZIP BDD100K IMAGES (100K)\n","# ============================================================================\n","# Unzip the official BDD100K 100k images with progress tracking\n","\n","import zipfile\n","from tqdm import tqdm\n","\n","print(\"\\n\" + \"=\" * 60)\n","print(\"UNZIPPING BDD100K IMAGES (100K)\")\n","print(\"=\" * 60)\n","\n","\n","def unzip_with_progress(zip_path, extract_to=\".\"):\n","    \"\"\"Unzip file with tqdm progress bar\"\"\"\n","    if not os.path.exists(zip_path):\n","        print(f\"âŒ File not found: {zip_path}\")\n","        print(f\"ðŸ“ Available files:\")\n","        get_ipython().system(\"ls -lh *.zip\")\n","        return False\n","\n","    print(f\"ðŸ“¦ Extracting: {zip_path}\")\n","    print(f\"ðŸ“‚ To: {extract_to}\")\n","\n","    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n","        file_list = zip_ref.namelist()\n","        total_files = len(file_list)\n","\n","        print(f\"ðŸ“Š Total files in archive: {total_files:,}\")\n","\n","        for file in tqdm(file_list, desc=\"Extracting\", unit=\"files\"):\n","            zip_ref.extract(file, extract_to)\n","\n","    print(f\"âœ“ Extraction complete!\")\n","    return True\n","\n","\n","# Create data directory\n","os.chdir(\"data\")\n","\n","# Unzip images (adjust filename if needed)\n","# Expected file: bdd100k_images_100k.zip (~36 GB)\n","images_zip = \"bdd100k_images_100k.zip\"\n","images_extract_dir = \"100k_images\"\n","os.makedirs(images_extract_dir, exist_ok=True)\n","if unzip_with_progress(images_zip, images_extract_dir):\n","    print(f\"âœ“ Images unzipped successfully to {images_extract_dir}/\")\n","else:\n","    print(\"âš ï¸  Please ensure bdd100k_images_100k.zip is in the project root\")\n","\n","print(\"\\nðŸ“ Current structure:\")\n","get_ipython().system(\"ls -lh\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M3u1PSg7GgSX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764743656852,"user_tz":-480,"elapsed":1786945,"user":{"displayName":"Lam Kingsley","userId":"13349381949172420159"}},"outputId":"04ccc1c1-901f-4fd4-8f49-484ec0d21b75"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","============================================================\n","UNZIPPING BDD100K LABELS\n","============================================================\n","ðŸ“¦ Extracting: bdd100k_labels.zip\n","ðŸ“‚ To: 100k_labels\n","ðŸ“Š Total files in archive: 100,004\n"]},{"output_type":"stream","name":"stderr","text":["Extracting: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100004/100004 [29:15<00:00, 56.97files/s]\n"]},{"output_type":"stream","name":"stdout","text":["âœ“ Extraction complete!\n","âœ“ Labels unzipped successfully to 100k_labels/\n","\n","ðŸ“ Final structure:\n",".:\n","total 5.5G\n","drwx------ 3 root root 4.0K Dec  3 05:34 100k_images\n","drwx------ 3 root root 4.0K Dec  3 06:04 100k_labels\n","-rw------- 1 root root 5.3G Dec  3 04:52 bdd100k_images_100k.zip\n","-rw------- 1 root root 181M Dec  3 04:42 bdd100k_labels.zip\n","\n","./100k_images:\n","total 4.0K\n","drwx------ 5 root root 4.0K Dec  3 05:59 100k\n","\n","./100k_images/100k:\n","total 12K\n","drwx------ 2 root root 4.0K Dec  3 06:03 test\n","drwx------ 2 root root 4.0K Dec  3 05:57 train\n","drwx------ 2 root root 4.0K Dec  3 05:59 val\n","\n","./100k_images/100k/test:\n","total 1.1G\n","-rw------- 1 root root  50K Dec  3 06:02 cabc30fc-e7726578.jpg\n","-rw------- 1 root root  56K Dec  3 06:01 cabc30fc-eb673c5a.jpg\n","-rw------- 1 root root  54K Dec  3 06:02 cabc30fc-fd79926f.jpg\n","-rw------- 1 root root  38K Dec  3 06:01 cabc9045-1b8282ba.jpg\n","-rw------- 1 root root  38K Dec  3 06:03 cabc9045-581f64de.jpg\n","-rw------- 1 root root  41K Dec  3 06:03 cabc9045-5a50690f.jpg\n","-rw------- 1 root root  37K Dec  3 06:00 cabc9045-b3349548.jpg\n","-rw------- 1 root root  49K Dec  3 05:59 cabc9045-c6dc9529.jpg\n","-rw------- 1 root root  35K Dec  3 06:02 cabc9045-cd422b81.jpg\n","-rw------- 1 root root  45K Dec  3 06:00 cabc9045-d91ecb66.jpg\n","-rw------- 1 root root  64K Dec  3 06:01 cabddb96-ca0ac856.jpg\n","-rw------- 1 root root  74K Dec  3 06:02 cabe1040-5f02711e.jpg\n","-rw------- 1 root root  49K Dec  3 06:03 cabe1040-c59cb390.jpg\n","-rw------- 1 root root  37K Dec  3 06:01 cabea010-6882cf41.jpg\n","-rw------- 1 root root  76K Dec  3 06:01 cabf7be1-36a39a28.jpg\n","-rw------- 1 root root  69K Dec  3 06:03 cabf7be1-f1a7e00d.jpg\n","-rw------- 1 root root  52K Dec  3 06:00 cabf9f3c-d58a6760.jpg\n","-rw------- 1 root root  69K Dec  3 05:59 cac07407-0396e053.jpg\n","-rw------- 1 root root  76K Dec  3 06:01 cac07407-0eb1c8bf.jpg\n","-rw------- 1 root root  68K Dec  3 06:03 cac07407-15b814db.jpg\n","-rw------- 1 root root  53K Dec  3 06:00 cac07407-196cd6f8.jpg\n","-rw------- 1 root root  50K Dec  3 06:03 cac07407-76e4c968.jpg\n","-rw------- 1 root root  61K Dec  3 06:02 cac07407-951977c8.jpg\n","-rw------- 1 root root  54K Dec  3 06:01 cac07407-ba37148a.jpg\n","-rw------- 1 root root  59K Dec  3 06:02 cac07407-bc0b048a.jpg\n","-rw------- 1 root root  49K Dec  3 06:00 cac07407-e969f06a.jpg\n","-rw------- 1 root root  70K Dec  3 06:02 cac07407-fe32e494.jpg\n","-rw------- 1 root root  71K Dec  3 06:00 cac32276-2ea56b83.jpg\n","-rw------- 1 root root  71K Dec  3 06:00 cac32276-a70feba7.jpg\n","-rw------- 1 root root  59K Dec  3 06:01 cac32276-cf233a28.jpg\n","-rw------- 1 root root  46K Dec  3 06:02 cac47e88-3227e13a.jpg\n"]}],"source":["# ============================================================================\n","# SECTION 4: UNZIP BDD100K LABELS\n","# ============================================================================\n","# Unzip the official BDD100K labels\n","\n","print(\"\\n\" + \"=\" * 60)\n","print(\"UNZIPPING BDD100K LABELS\")\n","print(\"=\" * 60)\n","\n","# Unzip labels (adjust filename if needed)\n","# Expected file: bdd100k_labels.zip (~2 GB)\n","labels_zip = \"bdd100k_labels.zip\"\n","labels_extract_dir = \"100k_labels\"\n","os.makedirs(labels_extract_dir, exist_ok=True)\n","if unzip_with_progress(labels_zip, labels_extract_dir):\n","    print(f\"âœ“ Labels unzipped successfully to {labels_extract_dir}/\")\n","else:\n","    print(\"âš ï¸  Please ensure bdd100k_labels.zip is in the project root\")\n","\n","print(\"\\nðŸ“ Final structure:\")\n","get_ipython().system(\"ls -lhR | head -50\")"]},{"cell_type":"markdown","source":["## 2. Filtering"],"metadata":{"id":"p180VQRECyqa"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Ipnaec-HhTf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764744143133,"user_tz":-480,"elapsed":7699,"user":{"displayName":"Lam Kingsley","userId":"13349381949172420159"}},"outputId":"d07a2ef1-9abd-45b1-8a61-9a92295b2bbd"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","============================================================\n","ANALYZING DIRECTORY STRUCTURE\n","============================================================\n","âœ“ Found training images at: 100k_images/100k/train\n","ðŸ“Š Found 70,000 training images\n","âœ“ Found training labels directory: 100k_labels/100k/train\n","  Contains 70,000 JSON files\n","\n","============================================================\n","DETECTED STRUCTURE:\n","============================================================\n","Images: 100k_images/100k/train\n","Labels: 100k_labels/100k/train (directory)\n","============================================================\n"]}],"source":["# ============================================================================\n","# SECTION 5: ANALYZE DIRECTORY STRUCTURE\n","# ============================================================================\n","# Identify the actual directory structure after extraction\n","\n","print(\"\\n\" + \"=\" * 60)\n","print(\"ANALYZING DIRECTORY STRUCTURE\")\n","print(\"=\" * 60)\n","\n","# Common possible structures after extraction\n","possible_image_paths = [\n","    \"100k_images/100k/train\",  # Actual structure from your extraction\n","    \"100k_images/bdd100k/images/100k/train\",\n","    \"100k_images/images/100k/train\",\n","]\n","\n","# Labels can be either a directory with individual JSON files or a single JSON file\n","possible_label_paths = [\n","    \"100k_labels/100k/train\",  # Directory with individual JSON files (your structure)\n","    \"100k_labels/labels/bdd100k_labels_images_train.json\",  # Single JSON file\n","    \"100k_labels/bdd100k_labels_images_train.json\",\n","]\n","\n","# Find images directory\n","train_img_dir = None\n","for path in possible_image_paths:\n","    if os.path.exists(path):\n","        train_img_dir = path\n","        print(f\"âœ“ Found training images at: {train_img_dir}\")\n","        break\n","\n","if train_img_dir is None:\n","    # Do a search\n","    print(\"ðŸ” Searching for image directories...\")\n","    get_ipython().system('find . -type d -name \"train\" | head -10')\n","    train_img_dir = input(\"Enter the correct path to train images directory: \")\n","\n","# Count images\n","if train_img_dir and os.path.exists(train_img_dir):\n","    train_images = [\n","        f\n","        for f in os.listdir(train_img_dir)\n","        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n","    ]\n","    print(f\"ðŸ“Š Found {len(train_images):,} training images\")\n","else:\n","    print(\"âŒ Training images directory not found!\")\n","\n","# Find labels (can be directory or single file)\n","train_label_path = None\n","labels_are_directory = False\n","\n","for path in possible_label_paths:\n","    if os.path.exists(path):\n","        train_label_path = path\n","        if os.path.isdir(path):\n","            labels_are_directory = True\n","            label_files = [f for f in os.listdir(path) if f.endswith('.json')]\n","            print(f\"âœ“ Found training labels directory: {train_label_path}\")\n","            print(f\"  Contains {len(label_files):,} JSON files\")\n","        else:\n","            print(f\"âœ“ Found training labels file: {train_label_path}\")\n","        break\n","\n","if train_label_path is None:\n","    # Do a search\n","    print(\"ðŸ” Searching for label files...\")\n","    get_ipython().system('find . -type f -name \"*train*.json\" | head -10')\n","    train_label_path = input(\"Enter the correct path to train labels: \")\n","    labels_are_directory = os.path.isdir(train_label_path)\n","\n","print(\"\\n\" + \"=\" * 60)\n","print(\"DETECTED STRUCTURE:\")\n","print(\"=\" * 60)\n","print(f\"Images: {train_img_dir}\")\n","print(f\"Labels: {train_label_path} ({'directory' if labels_are_directory else 'single file'})\")\n","print(\"=\" * 60)"]},{"cell_type":"code","source":["# ============================================================================\n","# SECTION 6: LOAD AND PARSE LABELS\n","# ============================================================================\n","# Load BDD100K labels and extract weather metadata\n","\n","import json\n","from glob import glob\n","\n","print(\"\\n\" + \"=\" * 60)\n","print(\"LOADING TRAINING LABELS\")\n","print(\"=\" * 60)\n","\n","all_labels = []\n","\n","if not os.path.exists(train_label_path):\n","    print(f\"âŒ ERROR: Label path not found at {train_label_path}\")\n","    print(\"Please update train_label_path variable with correct path\")\n","else:\n","    if labels_are_directory:\n","        # Load individual JSON files\n","        print(f\"ðŸ“¥ Loading individual JSON files from: {train_label_path}\")\n","        json_files = glob(os.path.join(train_label_path, \"*.json\"))\n","\n","        for json_file in tqdm(json_files, desc=\"Loading labels\"):\n","            with open(json_file, \"r\") as f:\n","                label_data = json.load(f)\n","                # Individual files have the structure directly\n","                all_labels.append(label_data)\n","\n","        print(f\"âœ“ Loaded {len(all_labels):,} individual label files\")\n","    else:\n","        # Load single JSON file with all labels\n","        print(f\"ðŸ“¥ Loading labels from single file: {train_label_path}\")\n","\n","        with open(train_label_path, \"r\") as f:\n","            all_labels = json.load(f)\n","\n","        # BDD100K format: list of image metadata\n","        if isinstance(all_labels, list):\n","            print(f\"âœ“ Loaded {len(all_labels):,} image labels\")\n","        else:\n","            print(f\"âš ï¸  Unexpected label format: {type(all_labels)}\")\n","\n","    # Show example\n","    if len(all_labels) > 0:\n","        example = all_labels[0]\n","        print(f\"\\nðŸ“‹ Example label structure:\")\n","        print(f\"  Name: {example.get('name', 'N/A')}\")\n","        print(f\"  Weather: {example.get('attributes', {}).get('weather', 'N/A')}\")\n","        print(f\"  Scene: {example.get('attributes', {}).get('scene', 'N/A')}\")\n","        print(f\"  Timeofday: {example.get('attributes', {}).get('timeofday', 'N/A')}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o_79sjoU84R_","executionInfo":{"status":"ok","timestamp":1764744639176,"user_tz":-480,"elapsed":434934,"user":{"displayName":"Lam Kingsley","userId":"13349381949172420159"}},"outputId":"be3ef40d-c951-45bb-e7cf-3d63348c7f8a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","============================================================\n","LOADING TRAINING LABELS\n","============================================================\n","ðŸ“¥ Loading individual JSON files from: 100k_labels/100k/train\n"]},{"output_type":"stream","name":"stderr","text":["Loading labels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 70000/70000 [07:12<00:00, 161.82it/s]"]},{"output_type":"stream","name":"stdout","text":["âœ“ Loaded 70,000 individual label files\n","\n","ðŸ“‹ Example label structure:\n","  Name: 6866acb3-cf17e759\n","  Weather: clear\n","  Scene: highway\n","  Timeofday: daytime\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["# ============================================================================\n","# SECTION 7: BUILD WEATHER MAPPING\n","# ============================================================================\n","# Create a dictionary mapping image names to weather conditions\n","\n","from collections import Counter\n","\n","print(\"\\n\" + \"=\" * 60)\n","print(\"BUILDING WEATHER MAP\")\n","print(\"=\" * 60)\n","\n","# Build weather map: image_name -> weather_type\n","weather_map = {}\n","\n","for item in tqdm(all_labels, desc=\"Processing labels\"):\n","    img_name = item[\"name\"] + \".jpg\"  # BDD100K images are .jpg\n","    weather = item.get(\"attributes\", {}).get(\"weather\", \"clear\")\n","    weather_map[img_name] = weather\n","\n","print(f\"âœ“ Built weather map for {len(weather_map):,} images\")\n","\n","# Show weather distribution\n","weather_counts = Counter(weather_map.values())\n","\n","print(\"\\nâ˜ï¸  Weather Distribution:\")\n","print(\"-\" * 60)\n","total = len(weather_map)\n","for weather, count in weather_counts.most_common():\n","    percentage = (count / total * 100) if total > 0 else 0\n","    bar_length = int(percentage / 2)  # Scale to 50 chars max\n","    bar = \"â–ˆ\" * bar_length + \"â–‘\" * (50 - bar_length)\n","    print(f\"  {weather:15s} â”‚ {bar} â”‚ {count:6,d} ({percentage:5.2f}%)\")\n","print(\"-\" * 60)\n","\n","# Key statistics\n","rainy_count = weather_counts.get(\"rainy\", 0)\n","clear_count = weather_counts.get(\"clear\", 0)\n","print(f\"\\nðŸŽ¯ Target counts:\")\n","print(f\"  Rainy images available: {rainy_count:,}\")\n","print(f\"  Clear images available: {clear_count:,}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gtyOh8Z2-x3q","executionInfo":{"status":"ok","timestamp":1764744722189,"user_tz":-480,"elapsed":618,"user":{"displayName":"Lam Kingsley","userId":"13349381949172420159"}},"outputId":"83ced88b-0579-4d34-f957-9cbc388dca2e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","============================================================\n","BUILDING WEATHER MAP\n","============================================================\n"]},{"output_type":"stream","name":"stderr","text":["Processing labels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 70000/70000 [00:00<00:00, 131469.46it/s]\n"]},{"output_type":"stream","name":"stdout","text":["âœ“ Built weather map for 70,000 images\n","\n","â˜ï¸  Weather Distribution:\n","------------------------------------------------------------\n","  clear           â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ â”‚ 37,412 (53.45%)\n","  overcast        â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ â”‚  8,784 (12.55%)\n","  undefined       â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ â”‚  8,134 (11.62%)\n","  snowy           â”‚ â–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ â”‚  5,571 ( 7.96%)\n","  rainy           â”‚ â–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ â”‚  5,083 ( 7.26%)\n","  partly cloudy   â”‚ â–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ â”‚  4,886 ( 6.98%)\n","  foggy           â”‚ â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ â”‚    130 ( 0.19%)\n","------------------------------------------------------------\n","\n","ðŸŽ¯ Target counts:\n","  Rainy images available: 5,083\n","  Clear images available: 37,412\n"]}]},{"cell_type":"code","source":["# ============================================================================\n","# SECTION 8: FILTER AND COLLECT RAINY IMAGES\n","# ============================================================================\n","# Copy rainy images to organized dataset structure\n","\n","import shutil\n","\n","print(\"\\n\" + \"=\" * 60)\n","print(\"COLLECTING RAINY IMAGES\")\n","print(\"=\" * 60)\n","\n","# Create output directories\n","output_dirs = {\n","    \"train_rainy\": \"train/rainy\",\n","    \"train_clear\": \"train/clear\",\n","    \"val_rainy\": \"val/rainy\",\n","    \"val_clear\": \"val/clear\",\n","    \"test_rainy\": \"test/rainy\",\n","    \"test_clear\": \"test/clear\",\n","}\n","\n","for folder in output_dirs.values():\n","    os.makedirs(folder, exist_ok=True)\n","\n","# Configuration\n","TARGET_TRAIN_RAINY = 1200\n","TARGET_TRAIN_CLEAR = 1200\n","TARGET_VAL_RAINY = 150\n","TARGET_VAL_CLEAR = 150\n","TARGET_TEST_RAINY = 200\n","TARGET_TEST_CLEAR = 200\n","\n","# Collect images\n","print(f\"ðŸŽ¯ Collection targets:\")\n","print(f\"  Training: {TARGET_TRAIN_RAINY:,} rainy + {TARGET_TRAIN_CLEAR:,} clear\")\n","print(f\"  Validation: {TARGET_VAL_RAINY:,} rainy + {TARGET_VAL_CLEAR:,} clear\")\n","print(f\"  Test: {TARGET_TEST_RAINY:,} rainy + {TARGET_TEST_CLEAR:,} clear\")\n","print()\n","\n","# Verify image directory exists\n","if not os.path.exists(train_img_dir):\n","    print(f\"âŒ ERROR: Image directory not found: {train_img_dir}\")\n","else:\n","    # Get all image files\n","    all_image_files = [\n","        f\n","        for f in os.listdir(train_img_dir)\n","        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n","    ]\n","\n","    print(f\"ðŸ“Š Scanning {len(all_image_files):,} images from {train_img_dir}\")\n","    print()\n","\n","    # Separate images by weather\n","    rainy_images = []\n","    clear_images = []\n","\n","    for img_name in tqdm(all_image_files, desc=\"Filtering by weather\"):\n","        weather = weather_map.get(img_name, \"unknown\")\n","\n","        if weather == \"rainy\":\n","            rainy_images.append(img_name)\n","        elif weather == \"clear\":\n","            clear_images.append(img_name)\n","\n","    print(f\"\\nâœ“ Filtered images:\")\n","    print(f\"  Rainy: {len(rainy_images):,}\")\n","    print(f\"  Clear: {len(clear_images):,}\")\n","\n","    # Shuffle for random distribution\n","    import random\n","\n","    random.seed(42)\n","    random.shuffle(rainy_images)\n","    random.shuffle(clear_images)\n","\n","    # Split into train/val/test\n","    # Rainy split\n","    rainy_train = rainy_images[:TARGET_TRAIN_RAINY]\n","    rainy_val = rainy_images[TARGET_TRAIN_RAINY : TARGET_TRAIN_RAINY + TARGET_VAL_RAINY]\n","    rainy_test = rainy_images[\n","        TARGET_TRAIN_RAINY + TARGET_VAL_RAINY : TARGET_TRAIN_RAINY\n","        + TARGET_VAL_RAINY\n","        + TARGET_TEST_RAINY\n","    ]\n","\n","    # Clear split\n","    clear_train = clear_images[:TARGET_TRAIN_CLEAR]\n","    clear_val = clear_images[TARGET_TRAIN_CLEAR : TARGET_TRAIN_CLEAR + TARGET_VAL_CLEAR]\n","    clear_test = clear_images[\n","        TARGET_TRAIN_CLEAR + TARGET_VAL_CLEAR : TARGET_TRAIN_CLEAR\n","        + TARGET_VAL_CLEAR\n","        + TARGET_TEST_CLEAR\n","    ]\n","\n","    print(f\"\\nðŸ“¦ Split distribution:\")\n","    print(f\"  Train: {len(rainy_train):,} rainy + {len(clear_train):,} clear\")\n","    print(f\"  Val:   {len(rainy_val):,} rainy + {len(clear_val):,} clear\")\n","    print(f\"  Test:  {len(rainy_test):,} rainy + {len(clear_test):,} clear\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PZFj1eli-84k","executionInfo":{"status":"ok","timestamp":1764744766811,"user_tz":-480,"elapsed":7655,"user":{"displayName":"Lam Kingsley","userId":"13349381949172420159"}},"outputId":"6a7d200e-f648-4794-cf3d-4c0fe45f2d47"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","============================================================\n","COLLECTING RAINY IMAGES\n","============================================================\n","ðŸŽ¯ Collection targets:\n","  Training: 1,200 rainy + 1,200 clear\n","  Validation: 150 rainy + 150 clear\n","  Test: 200 rainy + 200 clear\n","\n","ðŸ“Š Scanning 70,000 images from 100k_images/100k/train\n","\n"]},{"output_type":"stream","name":"stderr","text":["Filtering by weather: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 70000/70000 [00:00<00:00, 424326.52it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","âœ“ Filtered images:\n","  Rainy: 5,083\n","  Clear: 37,412\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","ðŸ“¦ Split distribution:\n","  Train: 1,200 rainy + 1,200 clear\n","  Val:   150 rainy + 150 clear\n","  Test:  200 rainy + 200 clear\n"]}]},{"cell_type":"markdown","source":["## 3. Saving"],"metadata":{"id":"jvH5B8b9DCSq"}},{"cell_type":"code","source":["# ============================================================================\n","# SECTION 9: COPY IMAGES TO ORGANIZED STRUCTURE\n","# ============================================================================\n","# Copy filtered images to train/val/test directories\n","\n","print(\"\\n\" + \"=\" * 60)\n","print(\"COPYING IMAGES TO ORGANIZED STRUCTURE\")\n","print(\"=\" * 60)\n","\n","\n","def copy_images(image_list, src_dir, dest_dir, desc):\n","    \"\"\"Copy images with progress bar using subprocess (faster on Google Drive)\"\"\"\n","    import subprocess\n","\n","    copied = 0\n","    for img_name in tqdm(image_list, desc=desc, unit=\"imgs\"):\n","        src_path = os.path.join(src_dir, img_name)\n","        dest_path = os.path.join(dest_dir, img_name)\n","\n","        if os.path.exists(src_path):\n","            try:\n","                # Use subprocess cp command (much faster on mounted drives)\n","                subprocess.run(['cp', src_path, dest_path], check=True, capture_output=True)\n","                copied += 1\n","            except subprocess.CalledProcessError:\n","                # Fallback to shutil if cp fails\n","                shutil.copy(src_path, dest_path)\n","                copied += 1\n","\n","    return copied\n","\n","\n","# Copy all splits\n","print(\"ðŸ“‹ Copying training set...\")\n","train_rainy_copied = copy_images(\n","    rainy_train, train_img_dir, \"train/rainy\", \"ðŸŒ§ï¸  Train rainy\"\n",")\n","train_clear_copied = copy_images(\n","    clear_train, train_img_dir, \"train/clear\", \"â˜€ï¸  Train clear\"\n",")\n","\n","print(\"\\nðŸ“‹ Copying validation set...\")\n","val_rainy_copied = copy_images(rainy_val, train_img_dir, \"val/rainy\", \"ðŸŒ§ï¸  Val rainy\")\n","val_clear_copied = copy_images(clear_val, train_img_dir, \"val/clear\", \"â˜€ï¸  Val clear\")\n","\n","print(\"\\nðŸ“‹ Copying test set...\")\n","test_rainy_copied = copy_images(\n","    rainy_test, train_img_dir, \"test/rainy\", \"ðŸŒ§ï¸  Test rainy\"\n",")\n","test_clear_copied = copy_images(\n","    clear_test, train_img_dir, \"test/clear\", \"â˜€ï¸  Test clear\"\n",")\n","\n","print(\"\\n\" + \"=\" * 60)\n","print(\"COPY SUMMARY\")\n","print(\"=\" * 60)\n","print(\n","    f\"Train: {train_rainy_copied:,} rainy + {train_clear_copied:,} clear = {train_rainy_copied + train_clear_copied:,}\"\n",")\n","print(\n","    f\"Val:   {val_rainy_copied:,} rainy + {val_clear_copied:,} clear = {val_rainy_copied + val_clear_copied:,}\"\n",")\n","print(\n","    f\"Test:  {test_rainy_copied:,} rainy + {test_clear_copied:,} clear = {test_rainy_copied + test_clear_copied:,}\"\n",")\n","print(\n","    f\"TOTAL: {train_rainy_copied + val_rainy_copied + test_rainy_copied + train_clear_copied + val_clear_copied + test_clear_copied:,} images\"\n",")\n","print(\"=\" * 60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QyFN3YjX_Hip","executionInfo":{"status":"ok","timestamp":1764745919220,"user_tz":-480,"elapsed":499244,"user":{"displayName":"Lam Kingsley","userId":"13349381949172420159"}},"outputId":"7ffe0bb3-2143-4c81-c6a5-ca6ce2202882"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","============================================================\n","COPYING IMAGES TO ORGANIZED STRUCTURE\n","============================================================\n","ðŸ“‹ Copying training set...\n"]},{"output_type":"stream","name":"stderr","text":["ðŸŒ§ï¸  Train rainy: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1200/1200 [07:42<00:00,  2.60imgs/s]\n","â˜€ï¸  Train clear: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1200/1200 [00:22<00:00, 52.85imgs/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","ðŸ“‹ Copying validation set...\n"]},{"output_type":"stream","name":"stderr","text":["ðŸŒ§ï¸  Val rainy: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [00:02<00:00, 52.19imgs/s]\n","â˜€ï¸  Val clear: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [00:03<00:00, 48.30imgs/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","ðŸ“‹ Copying test set...\n"]},{"output_type":"stream","name":"stderr","text":["ðŸŒ§ï¸  Test rainy: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:03<00:00, 54.87imgs/s]\n","â˜€ï¸  Test clear: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:04<00:00, 41.85imgs/s]"]},{"output_type":"stream","name":"stdout","text":["\n","============================================================\n","COPY SUMMARY\n","============================================================\n","Train: 1,200 rainy + 1,200 clear = 2,400\n","Val:   150 rainy + 150 clear = 300\n","Test:  200 rainy + 200 clear = 400\n","TOTAL: 3,100 images\n","============================================================\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["## 4. Compress extracted files"],"metadata":{"id":"bLRSpYDdKw0X"}},{"cell_type":"code","source":["# ============================================================================\n","# SECTION 10: CREATE SHAREABLE DATASET ARCHIVE\n","# ============================================================================\n","# Package organized dataset for teammates (simple zip file)\n","\n","import zipfile\n","\n","print(\"\\n\" + \"=\" * 60)\n","print(\"CREATING SHAREABLE ARCHIVE FOR TEAMMATES\")\n","print(\"=\" * 60)\n","\n","# Create compressed zip archive\n","archive_name = \"derrain_dataset.zip\"\n","print(f\"ðŸ“¦ Creating archive: {archive_name}\")\n","print(\"   This may take a few minutes...\")\n","\n","try:\n","    with zipfile.ZipFile(archive_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n","        print(\"   Adding train/ folder...\")\n","        for root, dirs, files in os.walk(\"train/\"):\n","            for file in tqdm(files, desc=\"   Compressing train\", leave=False):\n","                file_path = os.path.join(root, file)\n","                arcname = os.path.join(\"data\", file_path)\n","                zipf.write(file_path, arcname)\n","\n","        print(\"   Adding val/ folder...\")\n","        for root, dirs, files in os.walk(\"val/\"):\n","            for file in tqdm(files, desc=\"   Compressing val\", leave=False):\n","                file_path = os.path.join(root, file)\n","                arcname = os.path.join(\"data\", file_path)\n","                zipf.write(file_path, arcname)\n","\n","        print(\"   Adding test/ folder...\")\n","        for root, dirs, files in os.walk(\"test/\"):\n","            for file in tqdm(files, desc=\"   Compressing test\", leave=False):\n","                file_path = os.path.join(root, file)\n","                arcname = os.path.join(\"data\", file_path)\n","                zipf.write(file_path, arcname)\n","\n","        # Add metadata if it exists (created in Section 11)\n","        metadata_file = \"dataset_info.json\"\n","        if os.path.exists(metadata_file):\n","            print(\"   Adding metadata...\")\n","            zipf.write(metadata_file, \"data/dataset_info.json\")\n","        else:\n","            print(f\"   âš ï¸  Skipping metadata (not found at: {os.path.abspath(metadata_file)})\")\n","            print(f\"   â„¹ï¸  Metadata will be created in Section 11\")\n","\n","    # Calculate archive size\n","    archive_size_bytes = os.path.getsize(archive_name)\n","    archive_size_mb = archive_size_bytes / (1024 * 1024)\n","    archive_size_gb = archive_size_bytes / (1024 * 1024 * 1024)\n","\n","    print(f\"\\nâœ“ Archive created successfully!\")\n","    print(\"=\" * 60)\n","    print(f\"ðŸ“¦ Archive Details:\")\n","    print(f\"   File: {archive_name}\")\n","    if archive_size_gb >= 1.0:\n","        print(f\"   Size: {archive_size_gb:.2f} GB ({archive_size_mb:.0f} MB)\")\n","    else:\n","        print(f\"   Size: {archive_size_mb:.0f} MB\")\n","    print(f\"   Location: {os.path.join(os.getcwd(), archive_name)}\")\n","    print(\"=\" * 60)\n","\n","    print(f\"\\nðŸ“¤ Share this file with your teammates!\")\n","    print(f\"\\nðŸ”§ Extraction command for teammates:\")\n","    print(f\"   unzip {archive_name}\")\n","    print(f\"\\nðŸ“‹ Contents:\")\n","    print(f\"   â€¢ data/train/rainy/ (1,200 images)\")\n","    print(f\"   â€¢ data/train/clear/ (1,200 images)\")\n","    print(f\"   â€¢ data/val/rainy/ (150 images)\")\n","    print(f\"   â€¢ data/val/clear/ (150 images)\")\n","    print(f\"   â€¢ data/test/rainy/ (200 images)\")\n","    print(f\"   â€¢ data/test/clear/ (200 images)\")\n","    if os.path.exists(\"dataset_info.json\"):\n","        print(f\"   â€¢ data/dataset_info.json (metadata)\")\n","\n","except Exception as e:\n","    print(f\"\\nâŒ Error creating archive: {e}\")\n","    print(\"You can create it manually with:\")\n","    print(f\"   zip -r {archive_name} train/ val/ test/ dataset_info.json\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3fRBQt_PK4Fg","executionInfo":{"status":"ok","timestamp":1764749916901,"user_tz":-480,"elapsed":46494,"user":{"displayName":"Lam Kingsley","userId":"13349381949172420159"}},"outputId":"1809aa69-8721-4f16-8ada-ed12554de6b6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","============================================================\n","CREATING SHAREABLE ARCHIVE FOR TEAMMATES\n","============================================================\n","ðŸ“¦ Creating archive: derrain_dataset.zip\n","   This may take a few minutes...\n","   Adding train/ folder...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   Adding val/ folder...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   Adding test/ folder...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   âš ï¸  Skipping metadata (not found at: /content/drive/MyDrive/comp4471/project/data/dataset_info.json)\n","   â„¹ï¸  Metadata will be created in Section 11\n","\n","âœ“ Archive created successfully!\n","============================================================\n","ðŸ“¦ Archive Details:\n","   File: derrain_dataset.zip\n","   Size: 156 MB\n","   Location: /content/drive/MyDrive/comp4471/project/data/derrain_dataset.zip\n","============================================================\n","\n","ðŸ“¤ Share this file with your teammates!\n","\n","ðŸ”§ Extraction command for teammates:\n","   unzip derrain_dataset.zip\n","\n","ðŸ“‹ Contents:\n","   â€¢ data/train/rainy/ (1,200 images)\n","   â€¢ data/train/clear/ (1,200 images)\n","   â€¢ data/val/rainy/ (150 images)\n","   â€¢ data/val/clear/ (150 images)\n","   â€¢ data/test/rainy/ (200 images)\n","   â€¢ data/test/clear/ (200 images)\n"]}]},{"cell_type":"code","source":["# ============================================================================\n","# SECTION 11: SAVE DATASET METADATA\n","# ============================================================================\n","# Create metadata file with dataset statistics\n","\n","from datetime import datetime\n","\n","print(\"\\n\" + \"=\" * 60)\n","print(\"SAVING DATASET METADATA\")\n","print(\"=\" * 60)\n","\n","metadata = {\n","    \"dataset_name\": \"BDD100K DriveDeRain Dataset\",\n","    \"creation_date\": datetime.now().isoformat(),\n","    \"source\": \"BDD100K: Images 100K (Official)\",\n","    \"image_resolution\": \"1280x720 (original)\",\n","    \"splits\": {\n","        \"train\": {\n","            \"rainy\": train_rainy_copied,\n","            \"clear\": train_clear_copied,\n","            \"total\": train_rainy_copied + train_clear_copied,\n","        },\n","        \"val\": {\n","            \"rainy\": val_rainy_copied,\n","            \"clear\": val_clear_copied,\n","            \"total\": val_rainy_copied + val_clear_copied,\n","        },\n","        \"test\": {\n","            \"rainy\": test_rainy_copied,\n","            \"clear\": test_clear_copied,\n","            \"total\": test_rainy_copied + test_clear_copied,\n","        },\n","    },\n","    \"totals\": {\n","        \"rainy\": train_rainy_copied + val_rainy_copied + test_rainy_copied,\n","        \"clear\": train_clear_copied + val_clear_copied + test_clear_copied,\n","        \"total_images\": train_rainy_copied\n","        + val_rainy_copied\n","        + test_rainy_copied\n","        + train_clear_copied\n","        + val_clear_copied\n","        + test_clear_copied,\n","    },\n","    \"paths\": {\n","        \"source_images\": train_img_dir,\n","        \"source_labels\": train_label_path,\n","        \"output_root\": os.getcwd(),\n","    },\n","}\n","\n","# Save metadata\n","metadata_file = \"dataset_info.json\"\n","with open(metadata_file, \"w\") as f:\n","    json.dump(metadata, f, indent=2)\n","\n","print(f\"âœ“ Metadata saved to: {metadata_file}\")\n","\n","# Display metadata\n","print(\"\\n\" + \"=\" * 60)\n","print(\"DATASET METADATA\")\n","print(\"=\" * 60)\n","print(json.dumps(metadata, indent=2))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GT2Z4-TYK7L1","executionInfo":{"status":"ok","timestamp":1764749939191,"user_tz":-480,"elapsed":40,"user":{"displayName":"Lam Kingsley","userId":"13349381949172420159"}},"outputId":"0fd07d67-a848-459d-d5a3-bd9be86bbc0c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","============================================================\n","SAVING DATASET METADATA\n","============================================================\n","âœ“ Metadata saved to: dataset_info.json\n","\n","============================================================\n","DATASET METADATA\n","============================================================\n","{\n","  \"dataset_name\": \"BDD100K DriveDeRain Dataset\",\n","  \"creation_date\": \"2025-12-03T08:18:59.068185\",\n","  \"source\": \"BDD100K: Images 100K (Official)\",\n","  \"image_resolution\": \"1280x720 (original)\",\n","  \"splits\": {\n","    \"train\": {\n","      \"rainy\": 1200,\n","      \"clear\": 1200,\n","      \"total\": 2400\n","    },\n","    \"val\": {\n","      \"rainy\": 150,\n","      \"clear\": 150,\n","      \"total\": 300\n","    },\n","    \"test\": {\n","      \"rainy\": 200,\n","      \"clear\": 200,\n","      \"total\": 400\n","    }\n","  },\n","  \"totals\": {\n","    \"rainy\": 1550,\n","    \"clear\": 1550,\n","    \"total_images\": 3100\n","  },\n","  \"paths\": {\n","    \"source_images\": \"100k_images/100k/train\",\n","    \"source_labels\": \"100k_labels/100k/train\",\n","    \"output_root\": \"/content/drive/MyDrive/comp4471/project/data\"\n","  }\n","}\n"]}]},{"cell_type":"code","source":["# ============================================================================\n","# SECTION 12: VERIFY DATASET INTEGRITY\n","# ============================================================================\n","# Verify all images are readable and correctly organized\n","\n","from PIL import Image\n","\n","print(\"\\n\" + \"=\" * 60)\n","print(\"VERIFYING DATASET INTEGRITY\")\n","print(\"=\" * 60)\n","\n","\n","def verify_images(directory):\n","    \"\"\"Verify all images in directory are readable\"\"\"\n","    issues = []\n","    image_files = [\n","        f\n","        for f in os.listdir(directory)\n","        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n","    ]\n","\n","    for img_file in tqdm(\n","        image_files, desc=f\"Verifying {os.path.basename(directory)}\", unit=\"imgs\"\n","    ):\n","        img_path = os.path.join(directory, img_file)\n","        try:\n","            img = Image.open(img_path)\n","            img.verify()  # Verify it's a valid image\n","        except Exception as e:\n","            issues.append(f\"{img_file}: {str(e)}\")\n","\n","    return len(image_files), issues\n","\n","\n","# Verify all splits\n","total_verified = 0\n","total_issues = []\n","\n","for split in [\"train\", \"val\", \"test\"]:\n","    for condition in [\"rainy\", \"clear\"]:\n","        directory = f\"{split}/{condition}\"\n","        if os.path.exists(directory):\n","            count, issues = verify_images(directory)\n","            total_verified += count\n","            total_issues.extend(issues)\n","\n","print(f\"\\nâœ“ Verified {total_verified:,} images\")\n","\n","if total_issues:\n","    print(f\"âš ï¸  Found {len(total_issues)} issues:\")\n","    for issue in total_issues[:10]:  # Show first 10\n","        print(f\"  - {issue}\")\n","    if len(total_issues) > 10:\n","        print(f\"  ... and {len(total_issues) - 10} more\")\n","else:\n","    print(\"âœ“ All images are valid and readable!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5__uJ0zxLDE6","executionInfo":{"status":"ok","timestamp":1764749966118,"user_tz":-480,"elapsed":10676,"user":{"displayName":"Lam Kingsley","userId":"13349381949172420159"}},"outputId":"3b2f611f-e51a-47b7-9c5e-2b85d4408ca3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","============================================================\n","VERIFYING DATASET INTEGRITY\n","============================================================\n"]},{"output_type":"stream","name":"stderr","text":["Verifying rainy: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1200/1200 [00:03<00:00, 303.52imgs/s]\n","Verifying clear: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1200/1200 [00:04<00:00, 299.33imgs/s]\n","Verifying rainy: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [00:00<00:00, 229.37imgs/s]\n","Verifying clear: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [00:00<00:00, 317.94imgs/s]\n","Verifying rainy: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<00:00, 261.94imgs/s]\n","Verifying clear: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<00:00, 297.59imgs/s]"]},{"output_type":"stream","name":"stdout","text":["\n","âœ“ Verified 3,100 images\n","âœ“ All images are valid and readable!\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["# ============================================================================\n","# SECTION 13: DISPLAY FINAL SUMMARY\n","# ============================================================================\n","# Show final dataset organization and next steps\n","\n","print(\"\\n\" + \"=\" * 80)\n","print(\" \" * 25 + \"DATA PREPARATION COMPLETE!\")\n","print(\"=\" * 80)\n","\n","print(\"\\nðŸ“Š DATASET SUMMARY:\")\n","print(f\"  Total images: {metadata['totals']['total_images']:,}\")\n","print(f\"  Rainy images: {metadata['totals']['rainy']:,}\")\n","print(f\"  Clear images: {metadata['totals']['clear']:,}\")\n","\n","print(\"\\nðŸ“ DIRECTORY STRUCTURE:\")\n","print(f\"  {os.getcwd()}/\")\n","print(f\"    â”œâ”€â”€ train/\")\n","print(f\"    â”‚   â”œâ”€â”€ rainy/     ({train_rainy_copied:,} images)\")\n","print(f\"    â”‚   â””â”€â”€ clear/     ({train_clear_copied:,} images)\")\n","print(f\"    â”œâ”€â”€ val/\")\n","print(f\"    â”‚   â”œâ”€â”€ rainy/     ({val_rainy_copied:,} images)\")\n","print(f\"    â”‚   â””â”€â”€ clear/     ({val_clear_copied:,} images)\")\n","print(f\"    â”œâ”€â”€ test/\")\n","print(f\"    â”‚   â”œâ”€â”€ rainy/     ({test_rainy_copied:,} images)\")\n","print(f\"    â”‚   â””â”€â”€ clear/     ({test_clear_copied:,} images)\")\n","print(f\"    â””â”€â”€ dataset_info.json\")\n","\n","print(\"\\nðŸ“‹ FILES SAVED:\")\n","print(f\"  Dataset root: {os.getcwd()}\")\n","print(f\"  Metadata: {os.path.join(os.getcwd(), metadata_file)}\")\n","\n","print(\"\\nðŸŽ¯ NEXT STEPS:\")\n","print(\"  1. âœ“ Dataset preparation complete\")\n","print(\"  2. â†’ Generate synthetic rain (optional)\")\n","print(\"  3. â†’ Train derain model (Pix2Pix / Diffusion)\")\n","print(\"  4. â†’ Run evaluation (PSNR / SSIM / YOLO mAP)\")\n","\n","print(\"\\nðŸ’¡ USAGE:\")\n","print(\"  - Training pairs: use train/rainy as input, train/clear as target\")\n","print(\"  - Validation: use val/rainy and val/clear for monitoring\")\n","print(\"  - Testing: use test/rainy and test/clear for final evaluation\")\n","\n","print(\"\\n\" + \"=\" * 80)\n","print(\"Ready to proceed with model training! ðŸš€\")\n","print(\"=\" * 80)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TiGUrf3tLZfS","executionInfo":{"status":"ok","timestamp":1764750026638,"user_tz":-480,"elapsed":49,"user":{"displayName":"Lam Kingsley","userId":"13349381949172420159"}},"outputId":"20da167d-da91-46e6-cec8-1d60d3fe74ed"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","================================================================================\n","                         DATA PREPARATION COMPLETE!\n","================================================================================\n","\n","ðŸ“Š DATASET SUMMARY:\n","  Total images: 3,100\n","  Rainy images: 1,550\n","  Clear images: 1,550\n","\n","ðŸ“ DIRECTORY STRUCTURE:\n","  /content/drive/MyDrive/comp4471/project/data/\n","    â”œâ”€â”€ train/\n","    â”‚   â”œâ”€â”€ rainy/     (1,200 images)\n","    â”‚   â””â”€â”€ clear/     (1,200 images)\n","    â”œâ”€â”€ val/\n","    â”‚   â”œâ”€â”€ rainy/     (150 images)\n","    â”‚   â””â”€â”€ clear/     (150 images)\n","    â”œâ”€â”€ test/\n","    â”‚   â”œâ”€â”€ rainy/     (200 images)\n","    â”‚   â””â”€â”€ clear/     (200 images)\n","    â””â”€â”€ dataset_info.json\n","\n","ðŸ“‹ FILES SAVED:\n","  Dataset root: /content/drive/MyDrive/comp4471/project/data\n","  Metadata: /content/drive/MyDrive/comp4471/project/data/dataset_info.json\n","\n","ðŸŽ¯ NEXT STEPS:\n","  1. âœ“ Dataset preparation complete\n","  2. â†’ Generate synthetic rain (optional)\n","  3. â†’ Train derain model (Pix2Pix / Diffusion)\n","  4. â†’ Run evaluation (PSNR / SSIM / YOLO mAP)\n","\n","ðŸ’¡ USAGE:\n","  - Training pairs: use train/rainy as input, train/clear as target\n","  - Validation: use val/rainy and val/clear for monitoring\n","  - Testing: use test/rainy and test/clear for final evaluation\n","\n","================================================================================\n","Ready to proceed with model training! ðŸš€\n","================================================================================\n"]}]},{"cell_type":"markdown","source":["# Sync with Google Drive"],"metadata":{"id":"jxWsl8vungvI"}},{"cell_type":"code","source":["# Add to end of prepare.py\n","from google.colab import drive\n","import time\n","# Force sync\n","drive.flush_and_unmount()\n","time.sleep(5)\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bEs6ldzKnfru","executionInfo":{"status":"ok","timestamp":1764760936946,"user_tz":-480,"elapsed":5549135,"user":{"displayName":"Lam Kingsley","userId":"13349381949172420159"}},"outputId":"7b976824-8039-4909-ee7a-c8bb4e026a22"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]}],"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyMVFGQJXgo5niZZzWOmeL3N"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}